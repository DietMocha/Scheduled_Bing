{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Jume Notes:\n",
    "Basic neural network, understanding the math behind what is happening.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''https://www.youtube.com/watch?v=vcZub77WvFA'''\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# variables\n",
    "n_hidden = 10\n",
    "n_in = 10\n",
    "# output\n",
    "n_out = 10\n",
    "# sample data\n",
    "n_sample = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "#non deterministic seeding\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/njume/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/njume/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel/__main__.py:25: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:      nan, Time: 0.0223s\n",
      "Epoch: 1, Loss:      nan, Time: 0.0196s\n",
      "Epoch: 2, Loss:      nan, Time: 0.0205s\n",
      "Epoch: 3, Loss:      nan, Time: 0.0238s\n",
      "Epoch: 4, Loss:      nan, Time: 0.0225s\n",
      "Epoch: 5, Loss:      nan, Time: 0.0208s\n",
      "Epoch: 6, Loss:      nan, Time: 0.0191s\n",
      "Epoch: 7, Loss:      nan, Time: 0.0193s\n",
      "Epoch: 8, Loss:      nan, Time: 0.0195s\n",
      "Epoch: 9, Loss:      nan, Time: 0.0192s\n",
      "Epoch: 10, Loss:      nan, Time: 0.0215s\n",
      "Epoch: 11, Loss:      nan, Time: 0.0277s\n",
      "Epoch: 12, Loss:      nan, Time: 0.0231s\n",
      "Epoch: 13, Loss:      nan, Time: 0.0195s\n",
      "Epoch: 14, Loss:      nan, Time: 0.0195s\n",
      "Epoch: 15, Loss:      nan, Time: 0.0199s\n",
      "Epoch: 16, Loss:      nan, Time: 0.0204s\n",
      "Epoch: 17, Loss:      nan, Time: 0.0252s\n",
      "Epoch: 18, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 19, Loss:      nan, Time: 0.0225s\n",
      "Epoch: 20, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 21, Loss:      nan, Time: 0.0193s\n",
      "Epoch: 22, Loss:      nan, Time: 0.0220s\n",
      "Epoch: 23, Loss:      nan, Time: 0.0214s\n",
      "Epoch: 24, Loss:      nan, Time: 0.0214s\n",
      "Epoch: 25, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 26, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 27, Loss:      nan, Time: 0.0192s\n",
      "Epoch: 28, Loss:      nan, Time: 0.0208s\n",
      "Epoch: 29, Loss:      nan, Time: 0.0200s\n",
      "Epoch: 30, Loss:      nan, Time: 0.0215s\n",
      "Epoch: 31, Loss:      nan, Time: 0.0211s\n",
      "Epoch: 32, Loss:      nan, Time: 0.0193s\n",
      "Epoch: 33, Loss:      nan, Time: 0.0234s\n",
      "Epoch: 34, Loss:      nan, Time: 0.0229s\n",
      "Epoch: 35, Loss:      nan, Time: 0.0213s\n",
      "Epoch: 36, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 37, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 38, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 39, Loss:      nan, Time: 0.0206s\n",
      "Epoch: 40, Loss:      nan, Time: 0.0218s\n",
      "Epoch: 41, Loss:      nan, Time: 0.0189s\n",
      "Epoch: 42, Loss:      nan, Time: 0.0196s\n",
      "Epoch: 43, Loss:      nan, Time: 0.0205s\n",
      "Epoch: 44, Loss:      nan, Time: 0.0225s\n",
      "Epoch: 45, Loss:      nan, Time: 0.0215s\n",
      "Epoch: 46, Loss:      nan, Time: 0.0193s\n",
      "Epoch: 47, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 48, Loss:      nan, Time: 0.0190s\n",
      "Epoch: 49, Loss:      nan, Time: 0.0199s\n",
      "Epoch: 50, Loss:      nan, Time: 0.0191s\n",
      "Epoch: 51, Loss:      nan, Time: 0.0196s\n",
      "Epoch: 52, Loss:      nan, Time: 0.0190s\n",
      "Epoch: 53, Loss:      nan, Time: 0.0193s\n",
      "Epoch: 54, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 55, Loss:      nan, Time: 0.0225s\n",
      "Epoch: 56, Loss:      nan, Time: 0.0226s\n",
      "Epoch: 57, Loss:      nan, Time: 0.0212s\n",
      "Epoch: 58, Loss:      nan, Time: 0.0204s\n",
      "Epoch: 59, Loss:      nan, Time: 0.0200s\n",
      "Epoch: 60, Loss:      nan, Time: 0.0216s\n",
      "Epoch: 61, Loss:      nan, Time: 0.0209s\n",
      "Epoch: 62, Loss:      nan, Time: 0.0209s\n",
      "Epoch: 63, Loss:      nan, Time: 0.0206s\n",
      "Epoch: 64, Loss:      nan, Time: 0.0217s\n",
      "Epoch: 65, Loss:      nan, Time: 0.0218s\n",
      "Epoch: 66, Loss:      nan, Time: 0.0223s\n",
      "Epoch: 67, Loss:      nan, Time: 0.0239s\n",
      "Epoch: 68, Loss:      nan, Time: 0.0215s\n",
      "Epoch: 69, Loss:      nan, Time: 0.0225s\n",
      "Epoch: 70, Loss:      nan, Time: 0.0199s\n",
      "Epoch: 71, Loss:      nan, Time: 0.0228s\n",
      "Epoch: 72, Loss:      nan, Time: 0.0212s\n",
      "Epoch: 73, Loss:      nan, Time: 0.0212s\n",
      "Epoch: 74, Loss:      nan, Time: 0.0206s\n",
      "Epoch: 75, Loss:      nan, Time: 0.0221s\n",
      "Epoch: 76, Loss:      nan, Time: 0.0222s\n",
      "Epoch: 77, Loss:      nan, Time: 0.0203s\n",
      "Epoch: 78, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 79, Loss:      nan, Time: 0.0195s\n",
      "Epoch: 80, Loss:      nan, Time: 0.0197s\n",
      "Epoch: 81, Loss:      nan, Time: 0.0194s\n",
      "Epoch: 82, Loss:      nan, Time: 0.0205s\n",
      "Epoch: 83, Loss:      nan, Time: 0.0212s\n",
      "Epoch: 84, Loss:      nan, Time: 0.0208s\n",
      "Epoch: 85, Loss:      nan, Time: 0.0208s\n",
      "Epoch: 86, Loss:      nan, Time: 0.0273s\n",
      "Epoch: 87, Loss:      nan, Time: 0.0214s\n",
      "Epoch: 88, Loss:      nan, Time: 0.0256s\n",
      "Epoch: 89, Loss:      nan, Time: 0.0219s\n",
      "Epoch: 90, Loss:      nan, Time: 0.0224s\n",
      "Epoch: 91, Loss:      nan, Time: 0.0245s\n",
      "Epoch: 92, Loss:      nan, Time: 0.0228s\n",
      "Epoch: 93, Loss:      nan, Time: 0.0238s\n",
      "Epoch: 94, Loss:      nan, Time: 0.0231s\n",
      "Epoch: 95, Loss:      nan, Time: 0.0234s\n",
      "Epoch: 96, Loss:      nan, Time: 0.0242s\n",
      "Epoch: 97, Loss:      nan, Time: 0.0219s\n",
      "Epoch: 98, Loss:      nan, Time: 0.0211s\n",
      "Epoch: 99, Loss:      nan, Time: 0.0230s\n",
      "XOR prediction\n",
      "[0 0 0 0 1 1 0 1 1 1]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'atype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ce6e263720c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XOR prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ce6e263720c8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(x, V, W, bv, bw)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# create layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'atype'"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def train(x, t, V, W, bv, bw):\n",
    "    \n",
    "    # forward -- matrix multiply + biases\n",
    "    A = np.dot(x, V) + bv\n",
    "    Z = np.tanh(A)\n",
    "    \n",
    "    B = np.dot(Z, W)\n",
    "    Y = sigmoid(B)\n",
    "    \n",
    "    # backward\n",
    "    Ew = Y - t\n",
    "    Ev = tanh_prime(A) * np.dot(W, Ew)\n",
    "    \n",
    "    # predict our loss \n",
    "    dW = np.outer(Z, Ew)\n",
    "    dV = np.outer(x, Ev)\n",
    "    \n",
    "    # cross entropy\n",
    "    loss = -np.mean(t * np.log(Y) + (1-t) * np.log(1-Y))\n",
    "    \n",
    "    return loss, (dV, dW, Ev, Ew)\n",
    "\n",
    "def predict(x, V, W, bv, bw):\n",
    "    A = np.dot(x, V) + bv\n",
    "    B = np.dot(np.tanh(A), W) + bw\n",
    "    \n",
    "    return (sigmoid(B) > 0.5).atype(int)\n",
    "\n",
    "# create layers\n",
    "V = np.random.normal(scale=0.1, size = (n_in, n_hidden))\n",
    "W = np.random.normal(scale=0.1, size = (n_hidden, n_out))\n",
    "\n",
    "bv = np.zeros(n_hidden)\n",
    "bw = np.zeros(n_out)\n",
    "\n",
    "params = [V, W, bv, bw]\n",
    "\n",
    "#generate our data \n",
    "X = np.random.binomial(1, 0.5, (n_sample, n_in))\n",
    "T = X^1\n",
    "# training time\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0] * len(params)\n",
    "    t0 = time.clock()\n",
    "    \n",
    "    # for each data point, update our weights\n",
    "    for i in range(X.shape[0]):\n",
    "        loss, grad = train(X[i], T[i], * params)\n",
    "        \n",
    "        # update loss\n",
    "        for j in range(len(params)):\n",
    "            params[j] -= upd[j]\n",
    "            \n",
    "        for j in range(len(params)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum + upd[j]\n",
    "        \n",
    "        err.append(loss)\n",
    "        \n",
    "    print('Epoch: %d, Loss: %8f, Time: %.4fs' %(\n",
    "        epoch, np.mean(err), time.clock() - t0))\n",
    "\n",
    "# try to predict something\n",
    "x = np.random.binomial(1, 0.5, n_in)\n",
    "print('XOR prediction')\n",
    "print(x)\n",
    "print(predict(x, * params))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
